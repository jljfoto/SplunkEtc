### mlspl.conf #########
# This .conf file contains configuration for the "fit" and "apply"
# commands included with the Machine Learning Toolkit.
#
# Put global settings in the [default] stanza and algorithm-specific
# settings in a stanza named for the algorithm
# (e.g. [LinearRegression] for the LinearRegression algorithm).

[default]
# max_inputs specifies the maximum number of events an algorithm will
# consider when fitting a model. If this limit is exceeded and
# use_sampling is true, then the fit command will downsample its input
# using the "reservoir sampling" algorithm before fitting a model. If
# use_sampling is false and this limit is exceeded, the fit command
# will throw an error.
max_inputs = 100000

# Whether to use reservoir sampling for data sets that exceed
# max_inputs or to instead throw an error.
use_sampling = true

# Maximum time (in seconds) to spend in the "fit" phase of an
# algorithm (including down-sampling the input). This does not relate
# to the other phases of a search (e.g. retrieving events from an
# index).
max_fit_time = 600

# Maximum time (in seconds) to spend in the "score" phase of a scoring
# method(including down-sampling the input). This does not relate
# to the other phases of a search (e.g. retrieving events from an
# index).
max_score_time = 600

# Maximum allowed memory usage by the fit or apply commands (in
# megabytes) while fitting or applying a model, respectively.
max_memory_usage_mb = 1000

# Maximum allowed size of a model (in megabytes) created by the fit
# command. Some algorithms (e.g. SVM and RandomForest) may create
# unusually large models, which can lead to performance problems with
# bundle replication.
max_model_size_mb = 15

# Action to perform when new value(s) for categorical variable/explanatory variable is encountered in partial_fit
# default   : set all values of the column that corresponds to the new categorical value to 0's
# skip      : skip over rows that contain the new value(s) and raise a warning
# stop      : stop the operation by raising an error
handle_new_cat = default

# Setting streaming_apply to true allows the execution of apply command at indexer
# level. Otherwise apply is done on search head.
streaming_apply = false

# Maximum number of distinct values in categorical fields imposed 
# for one-hot encoding.
max_distinct_cat_values = 100

# Maximum number of distinct values in categorical fields imposed 
# for use in classifiers.
max_distinct_cat_values_for_classifiers = 100

# Maximum number of distinct values in categorical fields imposed
# for use in scoring methods.
max_distinct_cat_values_for_scoring = 100


### Algorithm-specific configuration
[Birch]
# Works well at 20000, but models are quite large.
max_inputs = 2000

[KernelPCA]
max_inputs = 5000

[OneClassSVM]
max_inputs = 10000

[SVM]
# Works well at 20000, but models are quite large.
max_inputs = 10000

[SpectralClustering]
# This algorithm is especially slow.
max_fit_time = 1800
max_inputs = 2000

[TFIDF]
max_inputs = 200000

[KernelRidge]
max_inputs = 5000

[DecisionTreeClassifier]
summary_depth_limit = 5
summary_return_json = false

[DecisionTreeRegressor]
summary_depth_limit = 5
summary_return_json = false

[ARIMA]
use_sampling = false

[score:classification]

[score:pairwise]
max_inputs = 1000

[score:regression]

[score:statstest]

[score:clustering]